{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    custom_infixes = [r'-']  # \"[a-z]+[\\\\-][\\n][a-z]+\"\n",
    "    prefix_re = spacy.util.compile_prefix_regex(list(nlp.Defaults.prefixes) + custom_infixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(list(nlp.Defaults.suffixes) + custom_infixes)\n",
    "    infix_re = spacy.util.compile_infix_regex(list(nlp.Defaults.infixes)+ custom_infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search,\n",
    "                     infix_finditer=infix_re.finditer, token_match=None)\n",
    "\n",
    "\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tmesis:\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        if not Token.has_extension(\"has_tmesis\"):\n",
    "            Token.set_extension(\"has_tmesis\", default=False)\n",
    "            Token.set_extension(\"tmesis_text\", default=\"\")\n",
    "        if not Token.has_extension(\"line\"):\n",
    "            Token.set_extension(\"line\", default=0)\n",
    "    def __call__(self, doc):\n",
    "        matcher = Matcher(doc.vocab)\n",
    "        matcher.add('tmesis', None, [\n",
    "            {\"TEXT\": {\"REGEX\": r\"[a-z침]+\"}},\n",
    "            {\"TEXT\": {\"REGEX\": r\"-$\"}},\n",
    "            {\"TEXT\": {\"REGEX\": r\"\\n+\"}},\n",
    "            {\"TEXT\": {\"REGEX\": r\"^[a-z침]+\"}},\n",
    "        ])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            lookup = self.nlp.Defaults.lemma_lookup\n",
    "            for _, start, end in matcher(doc):\n",
    "                span_text_raw = str(doc[start:end])\n",
    "                span_text = re.sub(r\"-\\n\", \"\", span_text_raw)\n",
    "                has_tmesis =  span_text in lookup.values() or span_text in lookup.keys()\n",
    "                attrs = {\n",
    "                    \"LEMMA\": lookup.get(span_text, span_text),\n",
    "                    \"_\": {\"has_tmesis\": has_tmesis, \"tmesis_text\": span_text}\n",
    "                }\n",
    "                retokenizer.merge(doc[start:end], attrs=attrs)\n",
    "        line_count = 0\n",
    "        for token in doc:\n",
    "            token._.line = line_count  # noqa\n",
    "            if '\\n' in token.text:\n",
    "                line_count += 1\n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe(\"tmesis\") if nlp.has_pipe(\"tmesis\") else None\n",
    "nlp.add_pipe(Tmesis(nlp), name=\"tmesis\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"El a침o 2000\n",
    "-a la luz de un candil-\n",
    "los supervivientes\n",
    "no vean m치s plan,\n",
    "que un terrible llan-\n",
    "to y crujir de dientes\n",
    "bellos se puede apos-\n",
    "tar,\n",
    "sin menospreciar\n",
    "a aquellos profetas,\n",
    "que aseguran que,\n",
    "el remedio viene\n",
    "de otros planetas\n",
    "malamente pero ok.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_tmesis_enjambment(previous_token, token, next_token):\n",
    "    \"\"\"\n",
    "\n",
    "    :param previous_token:\n",
    "    :param next_token:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return token._.has_tmesis  # noqa\n",
    "\n",
    "\n",
    "def get_sirrematic_enjambment(previous_token, next_token):\n",
    "    \"\"\"\n",
    "    Checks if sirrematic enjambment exists between two lines\n",
    "    :param previous_token: The word before a newline character\n",
    "    :param next_token: The word after a newline character\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sirremactic_pairs = [['ADJ', 'NOUN'],\n",
    "                         ['ADV', 'NOUN'],\n",
    "                         ['ADP', 'ADJ'],\n",
    "                         ['ADP', 'NOUN'],\n",
    "                         ['ADJ', 'ADV'],\n",
    "                         ['ADV', 'VERB']\n",
    "                         ]\n",
    "    while sirremactic_pairs:\n",
    "        sirrematic_pair = sirremactic_pairs.pop()\n",
    "        if sorted((previous_token.pos_, next_token.pos_)) == sorted(sirrematic_pair)\\\n",
    "                and (next_token.is_ancestor(previous_token) or previous_token.is_ancestor(next_token)):\n",
    "            return sirrematic_pair\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_enjambment(original_poem):\n",
    "    \"\"\"\n",
    "    Scan a text for all possible enjambment types.\n",
    "    :param original_poem:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    enjambment_types = ['sirrematic']\n",
    "    enjambments = {}\n",
    "    nlp_poem = nlp(original_poem)\n",
    "    # We iterate through all the text up to the penultimate line\n",
    "    for i, token in enumerate(nlp_poem[:-1]):\n",
    "        previous_token = nlp_poem[i - 1]\n",
    "        next_token = nlp_poem[i + 1]\n",
    "        # We look for enjambment when if there are words before and after a newline character\n",
    "        if token._.has_tmesis:  # noqa\n",
    "            print(token.text)\n",
    "            enjambments[token._.line] = ('tmesis', token.text.split('-\\n'))  # noqa\n",
    "            continue\n",
    "        elif token.text == '\\n' and not previous_token.is_punct and not next_token.is_punct:\n",
    "            for enjambment_type in enjambment_types:\n",
    "                enjambment_func = globals()[f'get_{enjambment_type}_enjambment']\n",
    "                enjambment = enjambment_func(previous_token, next_token)\n",
    "                if enjambment:\n",
    "                    enjambments[token._.line] = (enjambment_type, enjambment)  # noqa\n",
    "                    break\n",
    "    return enjambments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llan-\n",
      "to\n",
      "apos-\n",
      "tar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{4: ('tmesis', ['llan', 'to']),\n",
       " 6: ('tmesis', ['apos', 'tar']),\n",
       " 12: ('sirrematic', ['ADV', 'NOUN'])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_enjambment(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = nlp.Defaults.lemma_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'verdes' in lookup.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.es.Spanish"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
