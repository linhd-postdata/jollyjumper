{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    custom_infixes = [r'-']  # \"[a-z]+[\\\\-][\\n][a-z]+\"\n",
    "    prefix_re = spacy.util.compile_prefix_regex(list(nlp.Defaults.prefixes) + custom_infixes)\n",
    "    suffix_re = spacy.util.compile_suffix_regex(list(nlp.Defaults.suffixes) + custom_infixes)\n",
    "    infix_re  = spacy.util.compile_infix_regex(list(nlp.Defaults.infixes)+ custom_infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = \"\"\"el remedio tie-\n",
    "ne de otros pla-\n",
    "nos malamente\n",
    "pero eran ver-\n",
    "des y azules -y\n",
    "que otro sol oculta-\n",
    "Dos gemas muy bonitas.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tmesis:\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        if not Token.has_extension(\"has_tmesis\"):\n",
    "            Token.set_extension(\"has_tmesis\", default=False)\n",
    "            Token.set_extension(\"tmesis_text\", default=\"\")\n",
    "        if not Token.has_extension(\"verse\"):\n",
    "            Token.set_extension(\"verse\", default=0)\n",
    "    def __call__(self, doc):\n",
    "        matcher = Matcher(doc.vocab)\n",
    "        matcher.add('tmesis', None, [\n",
    "            {\"TEXT\": {\"REGEX\": r\"[a-zñ]+\"}},\n",
    "            {\"TEXT\": {\"REGEX\": r\"-$\"}},\n",
    "            {\"TEXT\": {\"REGEX\": r\"\\n+\"}},\n",
    "            {\"TEXT\": {\"REGEX\": r\"^[a-zñ]+\"}},\n",
    "        ])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for _, start, end in matcher(doc):\n",
    "                span_text_raw = str(doc[start:end])\n",
    "                span_text = re.sub(r\"-\\n\", \"\", span_text_raw)\n",
    "                attrs = {\n",
    "                    \"LEMMA\": self.nlp.Defaults.lemma_lookup.get(span_text, span_text_raw),\n",
    "                    \"_\": {\"has_tmesis\": True, \"tmesis_text\": span_text}\n",
    "                }\n",
    "                retokenizer.merge(doc[start:end], attrs=attrs)\n",
    "        verse_count = 0\n",
    "        for token in doc:\n",
    "            token._.verse = verse_count\n",
    "            if '\\n' in token.text:\n",
    "                verse_count += 1\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe(\"tmesis\") if nlp.has_pipe(\"tmesis\") else None\n",
    "nlp.add_pipe(Tmesis(nlp), name=\"tmesis\", first=True)\n",
    "# [(t.text, t.lemma_, t._.verse, t._.tmesis_text, t.tag_) for t in nlp(poem)]\n",
    "doc = nlp(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pos = nlp(' '.join([t._.tmesis_text or t.text for t in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('el', 'DET', 0, False, False),\n",
       " ('remedio', 'NOUN', 0, False, False),\n",
       " ('tie-\\nne', 'VERB', 0, True, True),\n",
       " ('de', 'ADP', 1, False, False),\n",
       " ('otros', 'DET', 1, False, False),\n",
       " ('pla-\\nnos', 'NOUN', 1, True, True),\n",
       " ('malamente', 'ADV', 2, False, False),\n",
       " ('\\n', 'SPACE', 2, True, False),\n",
       " ('pero', 'CONJ', 3, False, False),\n",
       " ('eran', 'AUX', 3, False, False),\n",
       " ('ver-\\ndes', 'ADJ', 3, True, True),\n",
       " ('y', 'CONJ', 4, False, False),\n",
       " ('azules', 'ADJ', 4, False, False),\n",
       " ('-', 'PUNCT', 4, False, False),\n",
       " ('y', 'CONJ', 4, False, False),\n",
       " ('\\n', 'SPACE', 4, True, False),\n",
       " ('que', 'PRON', 5, False, False),\n",
       " ('otro', 'DET', 5, False, False),\n",
       " ('sol', 'NOUN', 5, False, False),\n",
       " ('oculta', 'ADJ', 5, False, False),\n",
       " ('-', 'PUNCT', 5, False, False),\n",
       " ('\\n', 'SPACE', 5, True, False),\n",
       " ('Dos', 'NUM', 6, False, False),\n",
       " ('gemas', 'NOUN', 6, False, False),\n",
       " ('muy', 'ADV', 6, False, False),\n",
       " ('bonitas', 'ADJ', 6, False, False),\n",
       " ('.', 'PUNCT', 6, False, False)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for token, token_pos in zip(*[doc, doc_pos]):\n",
    "    token.pos = token_pos.pos\n",
    "    token.pos_ = token_pos.pos_\n",
    "    token.tag = token_pos.tag\n",
    "    token.tag_ = token_pos.tag_\n",
    "[(str(t),t.pos_, t._.verse, t.is_oov,t._.has_tmesis) for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1', 're', 'spacy', 'Matcher', 'Token', 'Tokenizer', 'custom_tokenizer', 'nlp', '_i2', 'poem', '_i3', 'Tmesis', '_i4', 'doc', '_i5', 'doc_pos', '_i6', 'token', 'token_pos', '_6', '_i7', '_exit_code', '_i8', '_i9', '_i10', '_i11', '_i12', '_i13', '_13', '_i14', '_i15', '_i16', '_i17', '_i18', '_i19', '_19', '_i20', '_i21', '_i22', '_i23', '_i24', '_i25', '_i26', '_26', '_i27', '_i28', '_i29', '_i30', '_i31', '_i32', '_32', '_i33', '_i34', '_i35', '_i36', '_36', '_i37', '_37', '_i38', '_38', '_i39', '_39', '_i40', '_40', '_i41', '_41', '_i42'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
